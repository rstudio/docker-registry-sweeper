#!/usr/bin/env python
# coding=utf-8
"""Docker Registry Sweeper (c) 2015 RStudio Inc.

Usage:
  docker-registry-sweeper [options] sweep
  docker-registry-sweeper [options] history repository <repository> 
  docker-registry-sweeper [options] history image <image>
  docker-registry-sweeper (-h | --help)
  docker-registry-sweeper --version

Options:
  -c --config=FILE          Path to configuration file. [default: conf/docker-registry-sweeper.conf]
  -g --graph=FILE           Load the graph from FILE.
  -f --graph-format=FORMAT  Graph format [default: json]
  -V --version              Show version.
  -v --verbose              Enable verbose output.

Options (sweep mode):
  -C --concurrency=NUM      Scan concurrency [default: 50]
  -e --blacklist=REPO       Exclude deleting images from the these repositories (comma separated list)
  -p --prune                Enabled repository pruning [default: False]
  -s --save=FILE            Save the graph to FILE (warning: this will overwrite).
  -a --age=AGE              Only remove images that have been unreferenced for at least AGE. [default: 1d]
  -d --delete               Delete images [default: False]
  -h --help                 Show this help.

Options (history mode):
  -u --unmarked             Show only unmarked images
"""
import collections
import contextlib
import fcntl
import functools
import json
import logging
import logging.config
import os
import re
from datetime import datetime, timedelta

import boto3
import dateutil.parser
import docopt
import networkx
import networkx.algorithms
import networkx.readwrite.gpickle
import networkx.readwrite.json_graph
import yaml
from tornado import escape
from tornado import gen
from tornado import ioloop
from tornado import locks
from tornado import queues

CONFIG = None


LOG = logging.getLogger('docker-registry-sweeper')


def retry(func=None, retries=3, delay=1, backoff=2, exceptions=(Exception,)):
    """
    A decorator for retrying coroutines
    :param func:
    :param retries: max number of retries
    :param delay: delay between retries (in seconds)
    :param backoff: backoff after retrying (in seconds)
    :param exceptions: exceptions to retry on
    :return: Future
    """

    if retries < 0:
        raise ValueError("Retries must be a positive integer.")

    def _wrap(f):
        @functools.wraps(f)
        def _wrapped(*args, **kwargs):
            result = None
            attempts = 0
            sleep = delay
            while True:
                try:
                    attempts += 1

                    # execute
                    ret = yield f(*args, **kwargs)

                except exceptions as ex:
                    # handle error

                    # raise keyboard interrupts or any exception that is not a subclass of the listed exceptions
                    if isinstance(ex, KeyboardInterrupt) or not any((issubclass(type(ex), t) for t in exceptions)):
                        # raise exception
                        result = ex
                        break

                    # log the exception
                    LOG.exception(ex)

                    if attempts <= retries:
                        LOG.warn("Retrying... (%d)", attempts)
                        yield gen.sleep(sleep)
                        sleep += backoff
                        continue
                    else:
                        # raise exception
                        result = ex
                        break

                else:
                    # no error
                    result = gen.Return(ret)
                    break

            # raise result
            if result is not None:
                raise result
            else:
                return

        return gen.coroutine(_wrapped)

    if func is not None:
        # Used like:
        # @retry
        return _wrap(func)
    else:
        # Used like @retry(whatever=whatever)
        return _wrap


def to_iso8601(when=None, tz=None):
    """
    Format a datetime as iso8601
    :param when: datetime (default to now)
    :param tz: localize to tz if no tzinfo
    :return: formatted datetime
    """
    if not when:
        if tz is None:
            when = datetime.utcnow()
        else:
            when = datetime.now(tz)
    if tz is not None and not when.tzinfo:
        when = tz.localize(when)
    _when = when.strftime("%Y-%m-%dT%H:%M:%S.%f%z")
    return _when


def from_iso8601(when, tz=None):
    """
    Parse an iso8601 formatted datetime
    :param when: formatted datetime
    :param tz: localize to tz if no tzinfo
    :return: datetime
    """
    _when = dateutil.parser.parse(when)
    if tz is not None and not _when.tzinfo:
        _when = tz.localize(_when)
    return _when


def parse_timedelta(v):
    """
    Parse a timedelta
    :param v:
    :return: timedelta
    >>> parse_timedelta('6m')
    datetime.timedelta(0, 360)
    >>> parse_timedelta('5s')
    datetime.timedelta(0, 5)
    >>> parse_timedelta('30')
    datetime.timedelta(0, 30)
    """

    def _to_seconds(s):
        seconds_per_unit = {"s": 1, "m": 60, "h": 3600, "d": 86400, "w": 604800}
        return int(s[:-1]) * seconds_per_unit[s[-1]]

    if re.match('^[0-9]+[s,m,h,d,w]$', v):
        return timedelta(seconds=_to_seconds(v))
    else:
        return timedelta(seconds=int(v))


class RegistryDriver(object):
    """
    An abstract class for storage drivers
    """

    @gen.coroutine
    def fetch_images(self, callback):
        raise NotImplementedError()

    @gen.coroutine
    def fetch_repositories(self, callback):
        raise NotImplementedError()

    @gen.coroutine
    def get_image_info(self, image):
        raise NotImplementedError()

    @gen.coroutine
    def get_image_ancestry(self, image):
        raise NotImplementedError()

    @gen.coroutine
    def get_repository_info(self, repository):
        raise NotImplementedError()

    @gen.coroutine
    def get_repository_tags(self, repository):
        raise NotImplementedError()

    @gen.coroutine
    def get_repository_index(self, repository):
        raise NotImplementedError()

    @gen.coroutine
    def delete_repository_tag(self, repository, tag):
        raise NotImplementedError()

    @gen.coroutine
    def delete_repository(self, repository):
        raise NotImplementedError()

    @gen.coroutine
    def delete_image(self, image):
        raise NotImplementedError()


class S3RegistryDriver(RegistryDriver):
    def __init__(self, bucket, path, region='us-east-1', access_key=None, secret_key=None, secure=True, io_loop=None):
        """
        Create a new instance of the S3RegistryDriver class
        :param bucket: Name of bucket
        :param path: Path prefix within bucket
        :param region: AWS region of bucket
        :param access_key: AWS Access Key ID. Falls back to boto3 strategies if None
        :param secret_key: AWS Secret Key.
        :param secure: If ``True``, use HTTPS, otherwise use HTTP.
        :param io_loop:
        :return:
        """

        self._resource = boto3.resource('s3', region_name=region,
                                        aws_access_key_id=access_key, aws_secret_access_key=secret_key, use_ssl=secure)
        self._client = boto3.client('s3', region_name=region,
                                    aws_access_key_id=access_key, aws_secret_access_key=secret_key, use_ssl=secure)
        self.bucket = self._resource.Bucket(bucket)

        if path:
            self.path = '%s/' % path
        else:
            self.path = ''

        self.secure = secure
        self._io_loop = io_loop or ioloop.IOLoop.current()

    def _list_bucket(self, prefix='', delimiter=''):
        """
        List the objects in a bucket for a given prefix
        :param prefix:
        :param delimiter:
        :return: an iterator of pages of results
        """
        paginator = self._client.get_paginator('list_objects_v2')
        return paginator.paginate(Bucket=self.bucket.name,
                                  Delimiter=delimiter,
                                  Prefix=prefix,
                                  PaginationConfig={'PageSize': 1000})

    @gen.coroutine
    def fetch_images(self, callback):
        """
        Fetch images from the registry and invoke callback for each
        :param callback:
        :return:
        """
        images = set()
        prefix = "{}images/".format(self.path)

        # list bucket
        pages = self._list_bucket(prefix=prefix, delimiter='/')

        # iterate common prefixes to get image ids
        for page in pages:
            for cp in page['CommonPrefixes']:
                if cp['Prefix'] != prefix:

                    # extract image id
                    image = cp['Prefix'][len(prefix):-1]

                    # since we're listing the bucket, duplicate prefixes may be returned
                    if image not in images:
                        images.add(image)

                        # fire callback
                        yield callback(image)

        LOG.debug("Found: %d images", len(images))

    @gen.coroutine
    def fetch_repositories(self, callback):
        """
        Fetch repositories from registry and invoke callback for each
        :param callback:
        :return:
        """
        repositories = set()
        prefix = "{}repositories/library/".format(self.path)

        # list bucket
        pages = self._list_bucket(prefix=prefix, delimiter='/')

        # iterate common prefixes to get repositories
        for page in pages:
            for cp in page['CommonPrefixes']:
                if cp['Prefix'] != prefix:

                    # extract repo name
                    repo = cp['Prefix'][len(prefix):-1]

                    # since we're listing the bucket, duplicate prefixes may be returned
                    if repo not in repositories:
                        repositories.add(repo)

                        # fire callback
                        yield callback(repo)

        LOG.debug("Found: %d repositories", len(repositories))

    @gen.coroutine
    def get_image_info(self, image):
        response = self.bucket.Object("{}images/{}/json".format(self.path, image)).get()
        raise gen.Return(escape.json_decode(response['Body'].read()))

    @gen.coroutine
    def get_image_ancestry(self, image):
        response = self.bucket.Object("{}images/{}/ancestry".format(self.path, image)).get()
        raise gen.Return(escape.json_decode(response['Body'].read()))

    @gen.coroutine
    def get_repository_info(self, repository):
        response = self.bucket.Object("{}repositories/library/{}/json".format(self.path, repository)).get()
        raise gen.Return(escape.json_decode(response['Body'].read()))

    @gen.coroutine
    def get_repository_index(self, repository):
        # get index url
        response = self.bucket.Object("{}repositories/library/{}/_index_images".format(self.path, repository)).get()

        raise gen.Return(escape.json_decode(response['Body'].read()))

    @gen.coroutine
    def get_repository_tags(self, repository):
        """
        Return a dict of tags for the given repository
        :param repository:
        :return:
        """
        tags = {}
        prefix = "{}repositories/library/{}/tag_".format(self.path, repository)
        pages = self._list_bucket(prefix=prefix)

        for page in pages:
            for content in page.get('Contents', []):
                # extract tag
                tag = content['Key'].split('/')[-1].split('_')[-1]

                # fetch image
                response = self.bucket.Object(content['Key']).get()
                image = response['Body'].read().decode('utf-8').replace('\"', '')

                tags[tag] = image

        raise gen.Return(tags)

    @gen.coroutine
    def delete_repository_tag(self, repository, tag):
        pass

    @gen.coroutine
    def delete_repository(self, repository):
        pass

    @gen.coroutine
    def delete_image(self, image):

        # delete only these keys
        keys = ['_files', '_checksum', 'ancestry', 'json', 'layer']

        for k in keys:
            self.bucket.Object("{}images/{}/{}".format(self.path, image, k)).delete()

            LOG.debug("Deleted: %s/%s", image, k)


class Registry(object):
    def __init__(self, driver):
        """
        Create a new instance of the Registry class
        :param driver:
        :return:
        """
        self.driver = driver
        self.graph = None

    def _read_graph(self, path, format='json'):
        LOG.debug("Reading graph...")
        graph = None
        with open(path, 'rb') as f:
            if format == 'pickle':
                graph = networkx.readwrite.gpickle.read_gpickle(path)
            elif format == 'json':
                graph = networkx.readwrite.json_graph.node_link_graph(json.load(f, 'utf-8'), directed=True)
            else:
                raise RuntimeError("Unsupported graph format: %s" % format)
        LOG.debug("Done!")
        return graph

    def _write_graph(self, graph, path, format='json', overwrite=False):
        LOG.debug("Writing graph...")
        if os.path.exists(path):
            if not overwrite:
                raise RuntimeError("Refusing to overwrite file: %s", path)
            else:
                LOG.warn("Overwriting file: %s", path)
        with open(path, 'wb') as f:
            if format == 'pickle':
                networkx.readwrite.gpickle.write_gpickle(graph, path)
            elif format == 'json':
                json.dump(networkx.readwrite.json_graph.node_link_data(graph), f, indent=4)
            else:
                raise RuntimeError("Unsupported graph format: %s" % format)
        LOG.debug("Done!")

    def load(self, path, format='json'):
        LOG.debug("Loading graph from: %s", path)
        self.graph = self._read_graph(path, format)

    @gen.coroutine
    def scan(self, save=None, save_format='json'):
        """
        Construct the registry graph by scanning
        :param save:
        :param save_format:
        :return:
        """

        # capture start time
        start_time = datetime.utcnow()

        # initialize the graph
        self.graph = networkx.DiGraph()

        # scan images
        yield self._scan_images()

        # scan repositories
        yield self._scan_repositories()

        # save graph
        if save is not None:
            self._write_graph(self.graph, save, save_format, True)

        LOG.info("Scan completed in %d seconds", (datetime.utcnow() - start_time).total_seconds())

    @gen.coroutine
    def _scan_images(self, concurrency=25):

        # initialize Queue
        queue = queues.Queue()
        semaphore = locks.BoundedSemaphore(concurrency)
        counters = collections.defaultdict(int)

        @gen.coroutine
        def _process_image():

            # get an image from the queue
            image = yield queue.get()

            try:

                LOG.debug("Processing: %s", image)

                # fetch the image ancestry
                ancestry = yield self.driver.get_image_ancestry(image)

                # add ancestor nodes
                LOG.debug("Adding image: %s", image)
                self.graph.add_node(image, tags=[], repos=[], marked=False)
                self.graph.add_path(ancestry)

                # output progress
                counters['images'] += 1
                if counters['images'] > 0 and counters['images'] % 100 == 0:
                    LOG.info("Processing images: complete=%d, queue=%d", counters['images'], queue.qsize())

            except:
                LOG.exception("Error processing image: %s", image)

                # put item back in the queue
                yield queue.put(image)

            finally:
                queue.task_done()
                semaphore.release()

        @gen.coroutine
        def _process_image_queue():

            # process forever
            while True:

                # acquire semaphore to limit concurrency
                yield semaphore.acquire()

                # process image
                _process_image()

        # start processing queue
        _process_image_queue()

        # fetch and queue images
        LOG.debug("Fetching images...")
        yield self.driver.fetch_images(queue.put)

        # wait for queue to be empty
        yield queue.join()

    @gen.coroutine
    def _scan_repositories(self, concurrency=25):

        # initialize Queue
        queue = queues.Queue()
        semaphore = locks.BoundedSemaphore(concurrency)
        counters = collections.defaultdict(int)

        @gen.coroutine
        def _process_repository():

            # get a repo from the queue
            repository = yield queue.get()

            try:

                # fetch repository index
                index = yield self.driver.get_repository_index(repository)
                if index is None:
                    LOG.warn("Repository %s index not found", repository)
                else:
                    for i in index:
                        try:
                            # mark indexed image
                            self.graph.node[i['id']]['repos'].append(repository)
                        except KeyError:
                            LOG.error("Indexed image not found in graph: %s (%s)", i['id'], repository)
                            continue

                # fetch repository tags
                tags = yield self.driver.get_repository_tags(repository)

                # mark each ancestor of the tagged image
                for tag, image in tags.iteritems():
                    LOG.debug("Processing tag: %s (%s/%s)", image, repository, tag)

                    try:
                        # mark tagged image
                        self.graph.node[image]['tags'].append("%s:%s" % (repository, tag))
                    except KeyError:
                        LOG.error("Tagged image not found in graph: %s (%s/%s)", image, repository, tag)
                        continue

                    # increment counter
                    counters['tags'] += 1

                    # do depth first search marking successor nodes
                    for n in networkx.algorithms.dfs_preorder_nodes(self.graph, image):

                        if not self.graph.node[n]['marked']:

                            LOG.debug("Marking: %s (%s/%s)", n, repository, tag)

                            # mark node
                            self.graph.node[n]['marked'] = True

                            # increment counter
                            counters['marked'] += 1

                        else:
                            # no need to continue, deeper nodes should already be marked
                            break

                # output progress
                counters['repositories'] += 1
                if counters['repositories'] > 0 and counters['repositories'] % 100 == 0:
                    LOG.info("Processing repositories: complete=%d, tags=%d, marked=%d, queue=%d",
                             counters['repositories'],
                             counters['tags'],
                             counters['marked'],
                             queue.qsize())

            except:
                LOG.exception("Error processing repository: %s", repository)

                # put item back in the queue
                queue.put(repository)

            finally:
                queue.task_done()
                semaphore.release()

        @gen.coroutine
        def _process_repository_queue():

            # process forever
            while True:

                # acquire semaphore to limit concurrency
                yield semaphore.acquire()

                # process repository
                _process_repository()

        # start processing queue
        _process_repository_queue()

        # fetch and queue repositories for processing
        LOG.debug("Fetching repositories...")
        yield self.driver.fetch_repositories(queue.put)

        # wait for queue to be empty
        yield queue.join()

    @gen.coroutine
    def image_history(self, image, filter_marked=False):

        LOG.debug("Getting image history: %s", image)

        history = []

        # construct tree containing all the descendants of image
        G = networkx.algorithms.dfs_tree(self.graph, image)

        # sort graph topologically producing an ordered list of nodes
        for n in networkx.algorithms.topological_sort(G):

            info = yield self.driver.get_image_info(n)

            image_marked = self.graph.node[n]['marked']

            if filter_marked and image_marked:
                continue

            image_tags = self.graph.node[n]['tags']
            image_repo = self.graph.node[n]['repos']
            image_date = from_iso8601(info['created'])
            image_size = info.get('Size', 0)

            image_cmd = None
            if 'container_config' in info:
                if info['container_config']['Cmd'] is not None:
                    image_cmd = ' '.join(info['container_config']['Cmd'])

            history.append(collections.OrderedDict(id=n,
                                                   command=image_cmd,
                                                   size=image_size,
                                                   repos=image_repo,
                                                   tags=image_tags,
                                                   marked=image_marked,
                                                   created=to_iso8601(image_date)))

        raise gen.Return(history)

    @gen.coroutine
    def repository_history(self, repository, filter_marked=False):

        LOG.debug("Getting repository history: %s", repository)

        # create graph for repositories
        repositories = networkx.DiGraph()

        # iterate the images graph to find unreferenced images
        for image, data in self.graph.nodes_iter(True):

            # for each repo the image is indexed in, insert into repositories graph
            tags = [(repo, tag) for repo,_,tag in [t.partition(':') for t in data['tags']]]
            for (repo, tag) in tags:
                repositories.add_edge(repo, image, tag=tag)

        # get image history
        history = {}

        if repository in repositories:
            for i in networkx.algorithms.descendants(repositories, repository):
                history[i] = yield self.image_history(i, filter_marked)

        raise gen.Return(history)

    def _read_sweep_file(self, path='delete.json'):
        # read sweep file
        if os.path.exists(path):
            LOG.debug("Reading sweep file: %s", path)
            with open(path, 'rb') as f:
                return {image: from_iso8601(date) for image, date in json.load(f, 'utf-8').iteritems()}
        return {}

    def _write_sweep_file(self, data, path='delete.json'):
        # write sweep file
        LOG.debug("Writing sweep file: %s", path)
        with open(path, 'wb') as f:
            json.dump({image: to_iso8601(date) for image, date in data.iteritems()}, f, encoding='utf-8')

    @gen.coroutine
    def _delete_image(self, image, dry_run):
        assert len(self.graph.predecessors(image)) == 0

        if dry_run:
            LOG.info("(Dry run) Deleting image: %s", image)
        else:
            LOG.info("Deleting image: %s", image)
            yield self.driver.delete_image(image)

        # remove the node from the graph
        self.graph.remove_node(image)

    @gen.coroutine
    def _prune_repository(self, repo, images, prune, dry_run):

        # gather a set of images to keep
        keep = images - prune

        LOG.info("Pruning repository: %s (removing: %s)", repo, ','.join(prune))

        # TODO: prune the repository

    @gen.coroutine
    def sweep(self, age, prune=False, blacklist=None, dry_run=True):
        """
        Sweep the graph for unreferenced images
        :param age:
        :param prune
        :param dry_run:
        :param blacklist
        :return:
        """

        blacklist = blacklist or []

        # create graph for repositories
        repositories = networkx.DiGraph()

        # iterate the images subgraph to find unreferenced images
        counter = 0
        unreferenced = set()
        for image, data in self.graph.nodes_iter(True):

            # check if image is tagged in a blacklisted repo
            blacklisted = set(data['repos']) & set(blacklist)
            if blacklisted:
                LOG.warn("Skipping image: %s (blacklisted: %s)", image, ','.join(blacklisted))
                continue

            # for each repo the image is indexed in, insert into repositories graph
            for repo in data['repos']:
                repositories.add_edge(repo, image)

            # if image is not marked, add to unreferenced
            if not data['marked']:
                unreferenced.add(image)
                LOG.debug("Unreferenced image: %s", image)

            # output progress
            counter += 1
            if counter % 1000 == 0:
                LOG.info("Analyzing images: complete=%d, unmarked=%d", counter, len(unreferenced))

        # read last sweep
        last_sweep = self._read_sweep_file()

        # gather a set of unreferenced images to delete
        current_sweep = dict()
        for image in unreferenced:

            # if we've already swept the image, use the existing date
            current_sweep[image] = last_sweep.get(image, datetime.utcnow())

        # find the intersection of current and last sweeps
        to_delete = set()
        for image in current_sweep.iterkeys():

            # only include images that have expired
            if datetime.utcnow() - current_sweep[image] >= age:
                to_delete.add(image)
            else:
                LOG.debug("Skipping image: %s (not old enough)", image)

        LOG.info("Found %d images to delete", len(to_delete))

        # delete the images in toposort order so that non-dependant images are deleted first
        deleted = set([])
        for image in networkx.algorithms.topological_sort(self.graph.subgraph(to_delete)):

            if prune:
                # if pruning is enabled, before deleting the image remove the image from the repository index

                # prune any repositories image is tagged in
                if image in repositories:

                    for repo in repositories.predecessors_iter(image):

                        # prune repository
                        yield self._prune_repository(repo, set(repositories.successors_iter(repo)), {image}, dry_run)

                    # remove image from repositories graph
                    repositories.remove_node(image)

            else:
                # if pruning is not enabled, skip the image entirely
                if image in repositories:
                    LOG.warn("Skipping image: %s (pruning disabled)", image)
                    continue

                # one last check before we delete
                assert len(self.graph.node[image]['tags']) == 0
                assert len(self.graph.node[image]['repos']) == 0

            # delete the image
            yield self._delete_image(image, dry_run)

            # add to deleted
            deleted.add(image)

        LOG.info("Deleted %d images", len(deleted))

        # save next sweep data
        next_sweep = {x:y for x,y in current_sweep.iteritems() if x not in deleted}
        self._write_sweep_file(next_sweep)


def init_config(configfile):
    global CONFIG
    with open(configfile, 'rb') as fh:
        CONFIG = yaml.load(fh)


def init_log(log_config, log_level=logging.INFO):
    logging.config.dictConfig(log_config)
    LOG.setLevel(log_level)


def load_driver():
    if CONFIG['registry']['driver'] == 's3':
        driver = S3RegistryDriver(CONFIG['registry']['bucket'],
                                  CONFIG['registry']['path'],
                                  CONFIG['registry'].get('region'),
                                  CONFIG['registry'].get('access_key'),
                                  CONFIG['registry'].get('secret_key'))
    else:
        raise ValueError("Unsupported registry driver: %s" % CONFIG['registry']['driver'])
    return driver


@contextlib.contextmanager
def pidfile(path=None):
    if path is None:
        path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'docker-registry-sweeper.pid')

    # write pidfile
    pidfile = open(path, "a+")
    try:
        # acquire file lock
        fcntl.flock(pidfile.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
    except IOError:
        raise SystemExit("Already running according to " + path)

    # write pid
    pidfile.seek(0)
    pidfile.truncate()
    pidfile.write(str(os.getpid()))
    pidfile.flush()
    pidfile.seek(0)
    LOG.debug("Wrote: %s", path)

    try:

        # return pidfile
        yield pidfile

    finally:

        # close pidfile
        try:
            pidfile.close()
        except IOError as err:
            # ok if file was just closed elsewhere
            if err.errno != 9:
                raise

        # cleanup pidfile
        os.remove(path)
        LOG.debug("Deleted: %s", path)


@gen.coroutine
def main():

    # initialize Driver
    driver = load_driver()

    # initialize Registry
    registry = Registry(driver)
    if arguments['--graph']:

        # load graph from file
        registry.load(arguments['--graph'], arguments['--graph-format'])
    else:

        # construct the graph by scanning the registry
        yield registry.scan(save=arguments['--save'],
                            save_format=arguments['--graph-format'])

    if arguments['sweep']:

        # run sweep mode
        yield registry.sweep(age=parse_timedelta(arguments['--age']),
                             prune=arguments['--prune'],
                             blacklist=arguments['--blacklist'].split(','),
                             dry_run=not arguments['--delete'])

    elif arguments['history']:

        if arguments['image']:

            # run history mode
            history = yield registry.image_history(arguments['<image>'],
                                                   filter_marked=arguments['--unmarked'])
            print json.dumps(history, indent=2)

        elif arguments['repository']:

            history = yield registry.repository_history(arguments['<repository>'],
                                                        filter_marked=arguments['--unmarked'])
            print json.dumps(history, indent=2)


if __name__ == "__main__":

    # parse arguments
    arguments = docopt.docopt(__doc__, version='0.1.0')

    # initialize config
    init_config(arguments['--config'])

    # initialize log
    init_log(CONFIG['logging'], logging.DEBUG if arguments['--verbose'] else logging.INFO)

    # write pid file
    with pidfile():

        # start ioloop
        ioloop.IOLoop().current().run_sync(main)
